{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-openai langchain langchain-community\n",
    "!pip install faiss-cpu\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run OpenAI compatible LLM server\n",
    "\n",
    "Execute `python -m llama_cpp.server --model <path-to-your-model>` \n",
    "\n",
    "For the tutorial I've used Llama 2 7b chat quantize model which you can find on HuggingFace:\n",
    "\n",
    "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the knowledge base\n",
    "\n",
    "Here we mock up a knowledge base by loading some information from a web page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://godofwar.fandom.com/wiki/Mimir\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the text\n",
    "\n",
    "Split the text into smaller chunks which represent the documents of the knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 832, which is longer than the specified 300\n",
      "Created a chunk of size 407, which is longer than the specified 300\n",
      "Created a chunk of size 420, which is longer than the specified 300\n",
      "Created a chunk of size 5162, which is longer than the specified 300\n",
      "Created a chunk of size 18039, which is longer than the specified 300\n",
      "Created a chunk of size 3824, which is longer than the specified 300\n",
      "Created a chunk of size 3224, which is longer than the specified 300\n",
      "Created a chunk of size 1666, which is longer than the specified 300\n",
      "Created a chunk of size 7296, which is longer than the specified 300\n",
      "Created a chunk of size 4208, which is longer than the specified 300\n",
      "Created a chunk of size 8208, which is longer than the specified 300\n",
      "Created a chunk of size 1796, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the vector store\n",
    "\n",
    "Create vectors from the documents using an embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "Let's use the OpenAI compatible LLM server to generate output using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", \n",
    "                  openai_api_base=\"http://localhost:8000/v1\",\n",
    "                  openai_api_key=\"sk-xxx\", \n",
    "                  max_tokens=2048,\n",
    "                  temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mimir (Nordic: ᛗᛁᛗᛁᚱ), formerly known as Puck, is a Celtic fae who became Odin's advisor and the ambassador of the Aesir Gods until Odin imprisoned him for 109 years. After being freed by Kratos and Atreus, he became their ally. He is the tritagonist of God of War (2018) and God of War Ragnarök.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the other name of Mimir?\"\n",
    "docs = db.similarity_search(question)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG based model prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "messages_rag = [\n",
    "    SystemMessage(\n",
    "\n",
    "        content=f\"\"\"You are a helpful assistant that answers questions based on a given context. You use only the current context for your answer. If the question cannot be answered directly from the context then output \"N/A\".\n",
    "                    Current context:\n",
    "                    {docs[0].page_content}\n",
    "                    \"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=question)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the other name of Mimir is Puck.\n"
     ]
    }
   ],
   "source": [
    "response = chat.invoke(messages_rag)\n",
    "print(response.content.lstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal model prompting (non RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_non_rag = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"You are a helpful assistant that answers questions.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=question),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, an excellent question! Mimir is also known as Mim to some people. It's a common nickname or shortened form of the name Mimir. So, you can use either \"Mimir\" or \"Mim\" interchangeably when referring to this fascinating being from Norse mythology!\n"
     ]
    }
   ],
   "source": [
    "response = chat.invoke(messages_non_rag)\n",
    "print(response.content.lstrip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diarizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
